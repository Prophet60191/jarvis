#!/usr/bin/env python3
"""
Test Quality Metrics System

Tests the comprehensive quality measurement system for RAG
including retrieval accuracy, answer faithfulness, and source citation.
"""

import sys
import asyncio
from pathlib import Path

# Add jarvis to path
sys.path.append(str(Path(__file__).parent / "jarvis"))


def test_quality_evaluator_initialization():
    """Test QualityEvaluator initialization."""
    print("üîß Testing Quality Evaluator Initialization")
    print("=" * 50)
    
    try:
        from jarvis.config import get_config
        from jarvis.tools.rag_service import RAGService
        from jarvis.tools.rag_quality_metrics import RAGQualityEvaluator
        
        config = get_config()
        rag_service = RAGService(config)
        
        # Initialize evaluator
        evaluator = RAGQualityEvaluator(rag_service)
        
        print("‚úÖ Quality evaluator initialized successfully")
        
        # Check attributes
        if hasattr(evaluator, 'rag_service'):
            print("‚úÖ RAG service reference set")
        else:
            print("‚ùå RAG service reference missing")
            return False
        
        if hasattr(evaluator, 'metrics_history'):
            print("‚úÖ Metrics history initialized")
        else:
            print("‚ùå Metrics history missing")
            return False
        
        if hasattr(evaluator, 'metrics_path'):
            print("‚úÖ Metrics path configured")
        else:
            print("‚ùå Metrics path missing")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Quality evaluator initialization failed: {e}")
        return False


def test_retrieval_accuracy_evaluation():
    """Test retrieval accuracy evaluation."""
    print("\nüìä Testing Retrieval Accuracy Evaluation")
    print("=" * 50)
    
    try:
        from jarvis.config import get_config
        from jarvis.tools.rag_service import RAGService
        from jarvis.tools.rag_quality_metrics import RAGQualityEvaluator
        
        config = get_config()
        rag_service = RAGService(config)
        evaluator = RAGQualityEvaluator(rag_service)
        
        # Create mock search results
        class MockDoc:
            def __init__(self, content, source):
                self.page_content = content
                self.metadata = {'source': source}
        
        mock_results = {
            'retrieved_documents': [
                MockDoc("Python is a programming language used for AI development", "python_guide.txt"),
                MockDoc("Machine learning algorithms require data preprocessing", "ml_basics.pdf"),
                MockDoc("Software engineering best practices include testing", "dev_guide.docx")
            ]
        }
        
        # Test retrieval accuracy
        query = "Python programming language"
        accuracy, details = evaluator.evaluate_retrieval_accuracy(
            query, mock_results, expected_sources=["python_guide.txt"]
        )
        
        print(f"‚úÖ Retrieval accuracy: {accuracy:.3f}")
        print(f"   Retrieved documents: {details['retrieved_count']}")
        print(f"   Relevant documents: {details['relevant_count']}")
        print(f"   Source diversity: {details['source_diversity']}")
        print(f"   Expected sources found: {details['expected_sources_found']}")
        
        # Validate results
        if 0 <= accuracy <= 1:
            print("‚úÖ Accuracy score in valid range")
        else:
            print(f"‚ùå Invalid accuracy score: {accuracy}")
            return False
        
        if details['retrieved_count'] == 3:
            print("‚úÖ Correct document count")
        else:
            print(f"‚ùå Incorrect document count: {details['retrieved_count']}")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Retrieval accuracy evaluation failed: {e}")
        return False


def test_answer_faithfulness_evaluation():
    """Test answer faithfulness evaluation."""
    print("\nüéØ Testing Answer Faithfulness Evaluation")
    print("=" * 50)
    
    try:
        from jarvis.config import get_config
        from jarvis.tools.rag_service import RAGService
        from jarvis.tools.rag_quality_metrics import RAGQualityEvaluator
        
        config = get_config()
        rag_service = RAGService(config)
        evaluator = RAGQualityEvaluator(rag_service)
        
        # Create mock documents and answer
        class MockDoc:
            def __init__(self, content):
                self.page_content = content
        
        mock_docs = [
            MockDoc("Python is a high-level programming language known for its simplicity"),
            MockDoc("It is widely used in artificial intelligence and machine learning")
        ]
        
        # Test faithful answer
        faithful_answer = "Python is a high-level programming language widely used in artificial intelligence"
        faithfulness, details = evaluator.evaluate_answer_faithfulness(
            "What is Python?", faithful_answer, mock_docs
        )
        
        print(f"‚úÖ Faithfulness score: {faithfulness:.3f}")
        print(f"   Content overlap: {details['source_content_overlap']:.3f}")
        print(f"   Factual consistency: {details['factual_consistency']:.3f}")
        print(f"   Hallucination indicators: {len(details['hallucination_indicators'])}")
        
        # Test unfaithful answer
        unfaithful_answer = "I think Python might be a snake, but I'm not sure about programming"
        unfaithful_score, unfaithful_details = evaluator.evaluate_answer_faithfulness(
            "What is Python?", unfaithful_answer, mock_docs
        )
        
        print(f"\nüìâ Unfaithful answer score: {unfaithful_score:.3f}")
        print(f"   Hallucination indicators: {len(unfaithful_details['hallucination_indicators'])}")
        
        # Validate results
        if faithfulness > unfaithful_score:
            print("‚úÖ Faithful answer scored higher than unfaithful answer")
        else:
            print("‚ùå Scoring issue: unfaithful answer scored higher")
            return False
        
        if len(unfaithful_details['hallucination_indicators']) > 0:
            print("‚úÖ Hallucination indicators detected in unfaithful answer")
        else:
            print("‚ùå Failed to detect hallucination indicators")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Answer faithfulness evaluation failed: {e}")
        return False


def test_source_citation_evaluation():
    """Test source citation accuracy evaluation."""
    print("\nüìö Testing Source Citation Evaluation")
    print("=" * 45)
    
    try:
        from jarvis.config import get_config
        from jarvis.tools.rag_service import RAGService
        from jarvis.tools.rag_quality_metrics import RAGQualityEvaluator
        
        config = get_config()
        rag_service = RAGService(config)
        evaluator = RAGQualityEvaluator(rag_service)
        
        # Create mock documents
        class MockDoc:
            def __init__(self, content, source):
                self.page_content = content
                self.metadata = {'source': source}
        
        mock_docs = [
            MockDoc("Python programming content", "python_guide.txt"),
            MockDoc("AI development content", "ai_handbook.pdf")
        ]
        
        # Test answer with good citations
        cited_answer = "According to python_guide.txt, Python is excellent for programming. Based on ai_handbook.pdf, it's also great for AI development."
        citation_accuracy, details = evaluator.evaluate_source_citation_accuracy(cited_answer, mock_docs)
        
        print(f"‚úÖ Citation accuracy: {citation_accuracy:.3f}")
        print(f"   Citations found: {details['citations_found']}")
        print(f"   Valid citations: {details['valid_citations']}")
        print(f"   Missing citations: {details['missing_citations']}")
        print(f"   Citation patterns: {details['citation_patterns']}")
        
        # Test answer without citations
        uncited_answer = "Python is great for programming and AI development."
        uncited_accuracy, uncited_details = evaluator.evaluate_source_citation_accuracy(uncited_answer, mock_docs)
        
        print(f"\nüìâ Uncited answer accuracy: {uncited_accuracy:.3f}")
        print(f"   Citations found: {uncited_details['citations_found']}")
        
        # Validate results
        if citation_accuracy > uncited_accuracy:
            print("‚úÖ Cited answer scored higher than uncited answer")
        else:
            print("‚ùå Citation scoring issue")
            return False
        
        if details['citations_found'] > 0:
            print("‚úÖ Citations detected in cited answer")
        else:
            print("‚ùå Failed to detect citations")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Source citation evaluation failed: {e}")
        return False


def test_comprehensive_quality_evaluation():
    """Test comprehensive quality evaluation."""
    print("\nüîç Testing Comprehensive Quality Evaluation")
    print("=" * 55)
    
    try:
        from jarvis.config import get_config
        from jarvis.tools.rag_service import RAGService
        from jarvis.tools.rag_quality_metrics import RAGQualityEvaluator
        
        config = get_config()
        rag_service = RAGService(config)
        evaluator = RAGQualityEvaluator(rag_service)
        
        # Add test memory
        test_fact = "Quality metrics testing: Python is the preferred programming language for AI projects"
        rag_service.add_conversational_memory(test_fact)
        
        print("üìù Added test memory for evaluation")
        
        # Run comprehensive evaluation
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            metrics = loop.run_until_complete(
                evaluator.evaluate_query("What programming language is preferred for AI?")
            )
            
            print(f"‚úÖ Comprehensive evaluation completed:")
            print(f"   Retrieval accuracy: {metrics.retrieval_accuracy:.3f}")
            print(f"   Answer faithfulness: {metrics.answer_faithfulness:.3f}")
            print(f"   Source citation accuracy: {metrics.source_citation_accuracy:.3f}")
            print(f"   Response time: {metrics.response_time:.2f}s")
            print(f"   Overall score: {metrics.overall_score:.3f}")
            
            # Validate metrics
            if 0 <= metrics.overall_score <= 1:
                print("‚úÖ Overall score in valid range")
            else:
                print(f"‚ùå Invalid overall score: {metrics.overall_score}")
                return False
            
            if metrics.response_time > 0:
                print("‚úÖ Response time recorded")
            else:
                print("‚ùå Invalid response time")
                return False
            
            # Test aggregate metrics
            aggregate = evaluator.get_aggregate_metrics()
            
            if aggregate['status'] == 'success':
                print("‚úÖ Aggregate metrics generated successfully")
                print(f"   Quality level: {aggregate['quality_level']}")
                print(f"   Evaluation count: {aggregate['evaluation_count']}")
            else:
                print("‚ùå Failed to generate aggregate metrics")
                return False
            
            return True
            
        finally:
            loop.close()
        
    except Exception as e:
        print(f"‚ùå Comprehensive quality evaluation failed: {e}")
        return False


def test_metrics_persistence():
    """Test metrics saving and loading."""
    print("\nüíæ Testing Metrics Persistence")
    print("=" * 40)
    
    try:
        from jarvis.config import get_config
        from jarvis.tools.rag_service import RAGService
        from jarvis.tools.rag_quality_metrics import RAGQualityEvaluator, QualityMetrics
        from datetime import datetime
        
        config = get_config()
        rag_service = RAGService(config)
        evaluator = RAGQualityEvaluator(rag_service)
        
        # Create test metrics
        test_metrics = QualityMetrics(
            retrieval_accuracy=0.85,
            answer_faithfulness=0.90,
            source_citation_accuracy=0.75,
            response_time=2.5,
            overall_score=0.83,
            timestamp=datetime.now().isoformat(),
            details={"test": "data"}
        )
        
        evaluator.metrics_history.append(test_metrics)
        
        # Test saving
        saved_path = evaluator.save_metrics("test_metrics.json")
        print(f"‚úÖ Metrics saved to: {saved_path}")
        
        # Test loading
        new_evaluator = RAGQualityEvaluator(rag_service)
        loaded_count = new_evaluator.load_metrics(saved_path)
        
        print(f"‚úÖ Loaded {loaded_count} metrics")
        
        if loaded_count == 1:
            loaded_metric = new_evaluator.metrics_history[0]
            if abs(loaded_metric.overall_score - test_metrics.overall_score) < 0.001:
                print("‚úÖ Metrics loaded correctly")
            else:
                print("‚ùå Metrics data mismatch")
                return False
        else:
            print("‚ùå Incorrect number of metrics loaded")
            return False
        
        # Cleanup
        import os
        if os.path.exists(saved_path):
            os.unlink(saved_path)
            print("‚úÖ Test file cleaned up")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Metrics persistence test failed: {e}")
        return False


def main():
    """Main test function."""
    print("üöÄ RAG Quality Metrics Testing Suite")
    print("=" * 60)
    print("Testing comprehensive quality measurement system...")
    print()
    
    tests = [
        ("Quality Evaluator Initialization", test_quality_evaluator_initialization),
        ("Retrieval Accuracy Evaluation", test_retrieval_accuracy_evaluation),
        ("Answer Faithfulness Evaluation", test_answer_faithfulness_evaluation),
        ("Source Citation Evaluation", test_source_citation_evaluation),
        ("Comprehensive Quality Evaluation", test_comprehensive_quality_evaluation),
        ("Metrics Persistence", test_metrics_persistence)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results.append((test_name, False))
    
    # Summary
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\nüìä Quality Metrics Test Results")
    print("=" * 40)
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    print(f"\nüìà Overall Results:")
    print(f"‚úÖ Passed: {passed}/{total}")
    print(f"‚ùå Failed: {total - passed}/{total}")
    print(f"üìä Success Rate: {passed/total*100:.1f}%")
    
    if passed == total:
        print("\nüéâ Quality Metrics System: COMPLETE!")
        print("   ‚úÖ Retrieval accuracy measurement working")
        print("   ‚úÖ Answer faithfulness evaluation functional")
        print("   ‚úÖ Source citation accuracy assessment operational")
        print("   ‚úÖ Comprehensive evaluation system working")
        print("   ‚úÖ Metrics persistence and aggregation successful")
        print("\nüìä RAG system now has comprehensive quality measurement!")
    elif passed >= total * 0.8:
        print(f"\n‚úÖ Quality Metrics System mostly complete!")
        print(f"   Core quality measurement working with minor issues")
    else:
        print(f"\n‚ö†Ô∏è  Quality Metrics System needs attention")
        print(f"   Multiple quality measurement issues detected")
    
    return passed >= total * 0.8


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
